#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åœ¨å¸¦äººå·¥æ ‡ç­¾å™ªå£°çš„ CIFAR-10 ä¸Šè®­ç»ƒ g_phiï¼ˆMLPFilterï¼‰ï¼Œä½œä¸ºâ€œå™ªå£°æ£€æµ‹å™¨â€ã€‚

è®¾è®¡æ€è·¯ï¼š
- Î¸_goodï¼šåœ¨å¹²å‡€ CIFAR-10 ä¸Šè®­ç»ƒå¥½çš„ ResNet-18ï¼ˆå‚è€ƒ train_theta_good_cifar10.pyï¼‰ï¼›
- æ•°æ®ï¼šåœ¨ train é›†ä¸Šæ³¨å…¥å¯¹ç§°æ ‡ç­¾å™ªå£°ï¼ˆnoise_rateï¼‰ï¼Œå¹¶è¿”å› is_noisy æ ‡è®°ï¼›
- ç‰¹å¾ zï¼š
    - loss_cls: per-sample CrossEntropy lossï¼›
    - å…¶å®ƒåˆ†é‡å¡« 0ï¼Œä½¿å¾— z ç»´åº¦ä¸º 5ï¼Œæ–¹ä¾¿ä¸æ£€æµ‹ç‰ˆä¿æŒä¸€è‡´ï¼›
    - conf_hard: 1 - max softmax probï¼Œä½œä¸ºâ€œæ ·æœ¬éš¾åº¦â€çš„ proxyï¼›
- ç›‘ç£ä¿¡å·ï¼š
    - target = 1ï¼ˆå¥½æ ·æœ¬ï¼‰è‹¥ is_noisy=Falseï¼›
    - target = 0ï¼ˆåæ ·æœ¬ï¼‰è‹¥ is_noisy=Trueï¼›
- æ¨¡å‹ï¼š
    - ä½¿ç”¨ ICML.core.yolo_bias_finetune.MLPFilter(input_dim=5) ä½œä¸º g_phiï¼›
- ç»“æœï¼š
    - å°† g_phi å‚æ•°ä¿å­˜åˆ° ICML/CIFAR10/Results/gphi_noise/noise_<rate>/seed_<seed>/mlpfilter_cifar10_noise.ptã€‚
"""

from __future__ import annotations

import argparse
import random
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from .config import get_default_cifar10_config
from .datasets import build_cifar10_dataloaders
from .models import build_resnet18_cifar10
from ICML.core.yolo_bias_finetune.mlp_filter import MLPFilter


def set_seed(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="åœ¨å¸¦äººå·¥æ ‡ç­¾å™ªå£°çš„ CIFAR-10 ä¸Šè®­ç»ƒ g_phiï¼ˆMLPFilter å™ªå£°æ£€æµ‹å™¨ï¼‰"
    )
    parser.add_argument("--seed", type=int, default=1088, help="éšæœºç§å­")
    parser.add_argument(
        "--noise-rate",
        type=float,
        default=0.4,
        help="train é›†æ ‡ç­¾å™ªå£°æ¯”ä¾‹ï¼ˆ0~1ï¼‰",
    )
    parser.add_argument("--epochs", type=int, default=5, help="g_phi è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=128, help="batch size")
    parser.add_argument("--device", type=str, default=None, help="è®¾å¤‡ï¼Œä¾‹å¦‚ 'cuda:0' æˆ– 'cpu'")
    parser.add_argument(
        "--theta-good-path",
        type=str,
        default=None,
        help="å¯é€‰ï¼šæŒ‡å®š Î¸_good æƒé‡è·¯å¾„ï¼ˆé»˜è®¤ä» CIFAR10/Results/theta_good_seed_xxx/best.pt æ¨å¯¼ï¼‰",
    )
    return parser.parse_args()


def build_theta_good(cfg, device: torch.device, seed: int, theta_good_path: Path | None) -> nn.Module:
    """
    æ„å»ºå¹¶åŠ è½½ Î¸_goodï¼ˆå¦‚å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä»…è¿”å›éšæœºåˆå§‹åŒ–çš„ ResNet-18ã€‚
    """
    model = build_resnet18_cifar10(num_classes=cfg.num_classes, device=device)

    if theta_good_path is None:
        theta_good_path = cfg.results_root / f"theta_good_seed_{seed}" / "best.pt"

    if theta_good_path.exists():
        state = torch.load(theta_good_path, map_location=device)
        model.load_state_dict(state)
        print(f"âœ… å·²ä» {theta_good_path} åŠ è½½ Î¸_good æƒé‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° Î¸_good æƒé‡ {theta_good_path}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ– ResNet-18")

    model.eval()
    for p in model.parameters():
        p.requires_grad_(False)
    return model


def main() -> None:
    args = parse_args()
    cfg = get_default_cifar10_config()

    set_seed(args.seed)

    device = torch.device(args.device) if args.device is not None else torch.device(
        "cuda:0" if torch.cuda.is_available() else "cpu"
    )

    # æ„å»ºå¸¦å™ªå£°æ ‡è®°çš„ CIFAR-10 dataloader
    dls = build_cifar10_dataloaders(
        data_root=str(cfg.data_root),
        batch_size=args.batch_size,
        num_workers=cfg.num_workers,
        noise_rate=args.noise_rate,
        seed=args.seed,
    )

    theta_good = build_theta_good(
        cfg,
        device=device,
        seed=args.seed,
        theta_good_path=Path(args.theta_good_path) if args.theta_good_path else None,
    )

    # g_phiï¼šè¾“å…¥ 5 ç»´ç‰¹å¾ z=[loss_cls,0,0,conf_hard,0]
    g_phi = MLPFilter(input_dim=5, device=device)
    g_phi.to(device)

    optimizer = torch.optim.Adam(g_phi.parameters(), lr=1e-3)
    bce = nn.BCELoss()

    out_root = cfg.results_root / "gphi_noise" / f"noise_{args.noise_rate:g}" / f"seed_{args.seed}"
    out_root.mkdir(parents=True, exist_ok=True)

    for epoch in range(args.epochs):
        g_phi.train()
        running_loss = 0.0
        n_samples = 0

        print(f"\n[Epoch {epoch + 1}/{args.epochs}] è®­ç»ƒ g_phi (noise_rate={args.noise_rate})")

        for imgs, labels, is_noisy in dls.train:
            imgs = imgs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            is_noisy = torch.as_tensor(is_noisy, device=device, dtype=torch.bool)

            with torch.no_grad():
                logits = theta_good(imgs)
                probs = F.softmax(logits, dim=1)
                conf, _ = probs.max(dim=1)
                loss_ce = F.cross_entropy(logits, labels, reduction="none")  # (B,)

            # æ„é€  5 ç»´ç‰¹å¾ï¼š
            # loss_cls = CE, å…¶ä½™ä¸¤ä¸ªæŸå¤±åˆ†é‡ç½® 0ï¼›conf_hard = 1 - confï¼›æœ€åä¸€ç»´å ä½ 0
            zeros = torch.zeros_like(loss_ce)
            conf_hard = 1.0 - conf
            z = torch.stack([loss_ce, zeros, zeros, conf_hard, zeros], dim=1)  # (B,5)
            z = torch.nan_to_num(z, nan=0.0, posinf=1.0, neginf=-1.0)

            # ç›‘ç£æ ‡ç­¾ï¼šclean=1, noisy=0
            target = (~is_noisy).float().view(-1, 1)

            optimizer.zero_grad()
            pred_weights = g_phi(z)  # (B,1)
            loss = bce(pred_weights, target)
            loss.backward()
            optimizer.step()

            bs = imgs.size(0)
            running_loss += float(loss.detach().cpu().item()) * bs
            n_samples += bs

        avg_loss = running_loss / max(n_samples, 1)
        print(f"âœ… Epoch {epoch + 1} å®Œæˆï¼ŒL_BCE(avg)={avg_loss:.4f}")

    out_path = out_root / "mlpfilter_cifar10_noise.pt"
    torch.save(g_phi.state_dict(), out_path)
    print(f"\nğŸ’¾ å·²ä¿å­˜ CIFAR-10 g_phi å™ªå£°æ£€æµ‹å™¨åˆ°: {out_path}")


if __name__ == "__main__":
    main()

