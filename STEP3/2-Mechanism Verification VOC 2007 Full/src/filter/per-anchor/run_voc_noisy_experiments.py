#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
VOC2007 å™ªå£°é›†å¯¹æ¯”å®éªŒå…¥å£ï¼ˆæ”¯æŒ per-sample ä¸ per-anchor ä¸¤ç§å™ªå£°ï¼‰ï¼š
- Baseline: ç›´æ¥åœ¨å™ªå£°é›†ä¸Šè®­ç»ƒ YOLOï¼ˆæ—  g_phiï¼‰
- Ours: åœ¨å™ªå£°é›†ä¸Šè®­ç»ƒï¼ŒåŠ è½½è§„åˆ™è’¸é¦å¾—åˆ°çš„ g_phiï¼Œå¯ç”¨è¿‡æ»¤

ç»“æœç›®å½•ï¼š
- Baseline: Results/Bias_only/<scenario>/seed_<seed>/results/...
- Ours:     Results/Bias+Filter_rule/<scenario>/seed_<seed>/results/...
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Dict, Any

from ICML.core.yolo_bias_finetune.anchor import AnchorModel
from ICML.core.yolo_bias_finetune.bias_trainer import BiasDetectionTrainer

# per-anchor è®­ç»ƒå™¨ï¼ˆé€ Anchor åŠ æƒï¼‰
PER_ANCHOR_TRAINER_PATH = Path(__file__).resolve().parent / "per_anchor_trainer.py"
if PER_ANCHOR_TRAINER_PATH.exists():
    sys.path.insert(0, str(PER_ANCHOR_TRAINER_PATH.parent))
    try:
        from per_anchor_trainer import PerAnchorBiasDetectionTrainer
    except Exception:
        PerAnchorBiasDetectionTrainer = None
else:
    PerAnchorBiasDetectionTrainer = None


DEFAULT_ANCHOR_DIR = (
    Path("/root/autodl-tmp/ICML/2-Mechanism Verification VOC 2007 Full/Results/Anchor/voc07_clean")
)
DEFAULT_DISTILL_DIR_MAP = {
    "per-sample": Path(
        "/root/autodl-tmp/ICML/2-Mechanism Verification VOC 2007 Full/Results/distill_rule_per-sample/voc07_noisy"
    ),
    "per-anchor": Path(
        "/root/autodl-tmp/ICML/2-Mechanism Verification VOC 2007 Full/Results/distill_rule_per-anchor/voc07_noisy"
    ),
}
DEFAULT_DATA_MAP = {
    "per-sample": Path("/root/autodl-tmp/dataset/voc07_noisy_per-sample/voc07_noisy.yaml"),
    "per-anchor": Path("/root/autodl-tmp/dataset/voc07_noisy_per-anchor/voc07_noisy.yaml"),
}


def build_overrides(
    data_yaml: Path,
    model_path: Path,
    project_dir: Path,
    epochs: int,
    batch: int,
    patience: int,
    lr0: float,
    optimizer: str,
    imgsz: int,
    seed: int,
    device: str,
    fraction: float,
) -> Dict[str, Any]:
    return {
        "task": "detect",
        "model": str(model_path),
        "data": str(data_yaml),
        "epochs": epochs,
        "batch": batch,
        "workers": 4,
        "patience": patience,
        "lr0": lr0,
        "optimizer": optimizer,
        "project": str(project_dir),
        "name": "results",
        "device": device,
        "imgsz": imgsz,
        "save": True,
        "save_period": -1,
        "verbose": True,
        "plots": True,
        "amp": True,
        "cache": False,
        "resume": False,
        "seed": seed,
        "deterministic": True,
        "fraction": fraction,
    }


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run VOC2007 noisy baseline vs g_phi (full) experiments")
    parser.add_argument(
        "--noise-mode",
        type=str,
        choices=["per-sample", "per-anchor"],
        default="per-sample",
        help="å™ªå£°æ¨¡å¼ï¼šper-sample=æ•´å›¾æ¯’ï¼Œper-anchor=æ··åˆ/å®ä¾‹å™ªå£°",
    )
    parser.add_argument(
        "--data",
        type=Path,
        default=None,
        help="voc07_noisy.yaml è·¯å¾„ï¼ˆä¸å¡«åˆ™æŒ‰ noise-mode ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰",
    )
    parser.add_argument(
        "--anchor",
        type=Path,
        default=None,
        help="å¹²å‡€ VOC è®­ç»ƒå¾—åˆ°çš„ anchor_voc_<model>.ptï¼ˆé»˜è®¤è‡ªåŠ¨åœ¨ Results/Anchor/voc07_clean ä¸‹å–æœ€æ–°ï¼‰",
    )
    parser.add_argument(
        "--mlp-filter",
        type=Path,
        default=None,
        help="è§„åˆ™è’¸é¦å¾—åˆ°çš„ g_phi æƒé‡è·¯å¾„ï¼ˆä¸å¡«åˆ™æŒ‰ noise-mode è‡ªåŠ¨é€‰æ‹©æœ€æ–°ï¼‰",
    )
    parser.add_argument(
        "--mlp-filter-dir",
        type=Path,
        default=None,
        help="æŒ‡å®šæŸ¥æ‰¾ mlp_filter çš„ç›®å½•ï¼ˆè¦†ç›– noise-mode é»˜è®¤ç›®å½•ï¼‰",
    )
    parser.add_argument("--epochs", type=int, default=10000, help="è®­ç»ƒ epochs")
    parser.add_argument("--batch", type=int, default=128, help="batch sizeï¼ˆé»˜è®¤ 128ï¼‰")
    parser.add_argument("--patience", type=int, default=20, help="early stop patience")
    parser.add_argument("--lr0", type=float, default=0.001, help="åˆå§‹å­¦ä¹ ç‡")
    parser.add_argument("--optimizer", type=str, default="AdamW", help="ä¼˜åŒ–å™¨")
    parser.add_argument("--imgsz", type=int, default=640, help="è¾“å…¥å°ºå¯¸")
    parser.add_argument("--seed", type=int, default=1088, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="0", help="YOLO deviceï¼Œå¦‚ '0' æˆ– 'cpu'")
    parser.add_argument(
        "--fraction",
        type=float,
        default=1.0,
        help="ä½¿ç”¨æ•°æ®é›†çš„æ¯”ä¾‹åšå¿«é€Ÿå†’çƒŸï¼ˆå¦‚ 0.01 è¡¨ç¤º 1% æ•°æ®ï¼‰",
    )
    parser.add_argument(
        "--warmup-epochs",
        type=int,
        default=0,
        help="per-anchor è¿‡æ»¤å¯ç”¨å‰çš„çƒ­èº«è½®æ•°ï¼ˆæƒé‡å…¨ä¸º 1ï¼Œä¸åšè¿‡æ»¤ï¼‰",
    )
    parser.add_argument(
        "--run-baseline",
        action="store_true",
        help="å¦‚éœ€åŒæ—¶è·‘ Baseline åˆ™åŠ æ­¤å‚æ•°ï¼›é»˜è®¤åªè·‘ g_phi ç‰ˆæœ¬",
    )
    parser.add_argument(
        "--results-root",
        type=Path,
        default=Path("/root/autodl-tmp/ICML/2-Mechanism Verification VOC 2007 Full/Results"),
        help="ç»“æœæ ¹ç›®å½•ï¼ˆä¼šåˆ›å»º Bias_only / Bias+Filter_rule å­ç›®å½•ï¼‰",
    )
    return parser.parse_args()


def train_once(
    data_yaml: Path,
    anchor_path: Path,
    mlp_filter_path: Path | None,
    use_filter: bool,
    overrides: Dict[str, Any],
    per_anchor: bool = False,
    per_anchor_kwargs: Dict[str, Any] | None = None,
) -> str:
    anchor = AnchorModel(anchor_path)
    if use_filter and per_anchor and PerAnchorBiasDetectionTrainer is not None:
        trainer = PerAnchorBiasDetectionTrainer(
            anchor_model=anchor,
            lambda_bias=0.0,
            mlpfilter_path=mlp_filter_path,
            **(per_anchor_kwargs or {}),
            overrides=overrides,
        )
    else:
        trainer = BiasDetectionTrainer(
            anchor_model=anchor,
            lambda_bias=0.0,  # ä¸åŠ  L_biasï¼Œåªç”¨ g_phi é‡åŠ æƒ
            use_sample_filter=use_filter,
            filter_mode="mlp",
            mlpfilter_init_path=mlp_filter_path if use_filter else None,
            overrides=overrides,
        )
    print(f"[trainer] Using {trainer.__class__.__name__}")
    trainer.train()
    return trainer.best


def main() -> None:
    args = parse_args()
    data_yaml = args.data or DEFAULT_DATA_MAP[args.noise_mode]
    if not data_yaml.exists():
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")
    # è‹¥æ— å¯ç”¨ GPUï¼Œè‡ªåŠ¨å›é€€åˆ° CPU
    try:
        import torch

        if not torch.cuda.is_available() and args.device != "cpu":
            args.device = "cpu"
    except Exception:
        args.device = "cpu"

    anchor_path = args.anchor
    if anchor_path is None:
        candidates = sorted(
            DEFAULT_ANCHOR_DIR.glob("anchor_voc_*.pt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
        if not candidates:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ° anchor æ¨¡å‹ï¼Œè¯·æŒ‡å®š --anchor æˆ–åœ¨ {DEFAULT_ANCHOR_DIR} ä¸‹æ”¾ç½® anchor_voc_*.pt"
            )
        anchor_path = candidates[0]
    if not anchor_path.exists():
        raise FileNotFoundError(f"anchor_voc.pt ä¸å­˜åœ¨: {anchor_path}")

    distill_dir = args.mlp_filter_dir or DEFAULT_DISTILL_DIR_MAP[args.noise_mode]
    mlp_filter_path = args.mlp_filter
    if mlp_filter_path is None:
        candidates = sorted(
            distill_dir.glob("mlp_filter_*.pt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
        if not candidates:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ° mlp_filter æ¨¡å‹ï¼Œè¯·æŒ‡å®š --mlp-filter æˆ–åœ¨ {distill_dir} ä¸‹æ”¾ç½® mlp_filter_*.pt"
            )
        mlp_filter_path = candidates[0]
    if mlp_filter_path is not None and not mlp_filter_path.exists():
        raise FileNotFoundError(f"mlp_filter_voc.pt ä¸å­˜åœ¨: {mlp_filter_path}")

    results_root = args.results_root
    results_root.mkdir(parents=True, exist_ok=True)
    scenario = f"voc07_noisy_{args.noise_mode}"

    print(f"ğŸ¯ VOC noisy experiment | noise_mode={args.noise_mode}")
    print(f"   data: {data_yaml}")
    print(f"   anchor: {anchor_path}")
    print(f"   mlp_filter_dir: {distill_dir}")
    print(f"   mlp_filter: {mlp_filter_path}")
    print(f"   results_root: {results_root}")
    print(f"   warmup_epochs (per-anchor): {args.warmup_epochs}")

    if args.run_baseline:
        base_root = results_root / "Bias_only" / scenario / f"seed_{args.seed}"
        base_root.mkdir(parents=True, exist_ok=True)
        base_overrides = build_overrides(
            data_yaml=data_yaml,
            model_path=anchor_path,
            project_dir=base_root,
            epochs=args.epochs,
            batch=args.batch,
            patience=args.patience,
            lr0=args.lr0,
            optimizer=args.optimizer,
            imgsz=args.imgsz,
            seed=args.seed,
            device=args.device,
            fraction=args.fraction,
        )
        print(f"\n=== Baseline: YOLO on {scenario} (no g_phi) ===")
        best_baseline = train_once(
            data_yaml=data_yaml,
            anchor_path=anchor_path,
            mlp_filter_path=None,
            use_filter=False,
            overrides=base_overrides,
        )
        print(f"Baseline best checkpoint: {best_baseline}")
    else:
        print("\n=== ä»…è¿è¡Œ g_phi ç‰ˆæœ¬ï¼ˆBaseline å·²è·³è¿‡ï¼Œè‹¥éœ€ Baseline è¯·åŠ  --run-baselineï¼‰ ===")

    # Ours: load mlp_filter_voc.pt and enable g_phi
    ours_root = results_root / "Bias+Filter_rule" / scenario / f"seed_{args.seed}"
    ours_root.mkdir(parents=True, exist_ok=True)
    ours_overrides = build_overrides(
        data_yaml=data_yaml,
        model_path=anchor_path,
        project_dir=ours_root,
        epochs=args.epochs,
        batch=args.batch,
        patience=args.patience,
        lr0=args.lr0,
        optimizer=args.optimizer,
        imgsz=args.imgsz,
            seed=args.seed,
            device=args.device,
            fraction=args.fraction,
        )
    per_anchor_mode = args.noise_mode == "per-anchor"
    per_anchor_kwargs = {
        "loss_thres": 0.5,  # æ”¾å®½é˜ˆå€¼ï¼Œå‡å°‘è¿‡åº¦åˆ¤å
        "bad_weight": 0.0,
        "topk": 500,
        "pos_scale": 2.0,  # å¼ºåŒ–å¥½æ ·æœ¬æƒé‡ï¼Œé¿å…å…¨ä½“è¢«å‹åˆ°ä¸‹ç•Œ
        "neg_scale": 1.0,
        "warmup_epochs": args.warmup_epochs,
    }
    print(f"\n=== Ours: YOLO on {scenario} with g_phi ({mlp_filter_path.name}) ===")
    best_ours = train_once(
        data_yaml=data_yaml,
        anchor_path=anchor_path,
        mlp_filter_path=mlp_filter_path,
        use_filter=True,
        overrides=ours_overrides,
        per_anchor=per_anchor_mode,
        per_anchor_kwargs=per_anchor_kwargs if per_anchor_mode else None,
    )
    print(f"Ours best checkpoint: {best_ours}")


if __name__ == "__main__":
    main()
