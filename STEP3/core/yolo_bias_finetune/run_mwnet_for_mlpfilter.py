#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŸºäº Meta-Weight-Net æ€æƒ³çš„ MLP FiLTER (g_phi) å¤–å±‚è®­ç»ƒè„šæœ¬ï¼ˆé˜¶æ®µäºŒï¼‰

ç›®æ ‡ï¼š
- åœ¨å›ºå®š YOLO + L_bias æ£€æµ‹æ¨¡å‹å‚æ•° Î¸ çš„å‰æä¸‹ï¼Œåªä¼˜åŒ–æ ·æœ¬è¿‡æ»¤å™¨ g_phiï¼ˆMLP FiLTERï¼‰çš„å‚æ•° Ï†ï¼›
- ä½¿ç”¨ç›®æ ‡åŸŸçš„éªŒè¯é›†ï¼ˆval / testï¼‰ä½œä¸ºâ€œmeta é›†â€ï¼Œè®© g_phi å­¦ä¹ ä¸€å¥—æƒé‡ w_i(Ï†)ï¼Œ
  ä½¿å¾—åœ¨ 1-step è™šæ‹Ÿæ›´æ–° Î¸' ä¹‹åï¼Œmeta é›†ä¸Šçš„æ£€æµ‹æŸå¤±å°½å¯èƒ½å°ï¼ˆåŒæ—¶é€šè¿‡è½»å¾®æ­£åˆ™ä¿æŒ w_i ä¸åç¦» 1ï¼‰ã€‚

å®ç°è¦ç‚¹ï¼ˆMWNet-style, 1-stepï¼‰ï¼š
- Inner loopï¼šåœ¨ train loader ä¸Šï¼Œç”¨å½“å‰ g_phi äº§ç”Ÿçš„ w_i å¯¹æ£€æµ‹ loss åŠ æƒï¼Œè®¡ç®— L_train(Î¸, Ï†)ï¼Œ
  å¯¹ Î¸ æ±‚æ¢¯åº¦å¹¶æ„é€ ä¸€æ¬¡è™šæ‹Ÿæ›´æ–° Î¸' = Î¸ - Î± âˆ‚L_train/âˆ‚Î¸ï¼›
- Outer loopï¼šåœ¨ meta loaderï¼ˆval/testï¼‰ä¸Šï¼Œç”¨ Î¸' è®¡ç®— L_meta(Î¸')ï¼Œå¹¶å¯¹ Ï† åå‘ä¼ æ’­ï¼Œ
  æ¢¯åº¦é“¾è·¯ä¸º L_meta(Î¸'(Ï†)) â†’ Ï†ï¼Œåªæ›´æ–° Ï†ï¼Œä¸æ›´æ–° Î¸ æœ¬èº«ã€‚

è®­ç»ƒå®Œæˆåï¼Œä¼šå°† MLP FiLTER çš„å‚æ•°ä¿å­˜åˆ°
    Toy/Results/Bias+Filter/<scenario>/seed_<seed>/mlpfilter_meta.pt
åç»­é˜¶æ®µä¸‰å¯ä»¥åœ¨ YOLO + L_bias + g_phi è®­ç»ƒæ—¶åŠ è½½è¯¥æƒé‡ä½œä¸ºåˆå§‹åŒ–ã€‚
"""

from __future__ import annotations

import argparse
from pathlib import Path
import sys

import torch
from torch.nn.utils.stateless import functional_call

from ultralytics.models.yolo.detect import DetectionTrainer

# å°† Baseline æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ ICML åŒ…
THIS_FILE = Path(__file__).resolve()
ICML_ROOT = THIS_FILE.parents[2]  # .../Baseline/ICML
BASELINE_ROOT = ICML_ROOT.parent
if str(BASELINE_ROOT) not in sys.path:
    sys.path.insert(0, str(BASELINE_ROOT))

from ICML.mt.config import build_bias_finetune_config
from ICML.core.yolo_bias_finetune.mlp_filter import MLPFilter, PerSampleWeightedDetectionLoss
from ICML.core.yolo_bias_finetune.train_bias_yolo import build_overrides


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="é˜¶æ®µäºŒï¼šåœ¨å›ºå®š Î¸ ä¸Šï¼Œç”¨ meta é›†è®­ç»ƒ MLP FiLTER (g_phi)"
    )
    parser.add_argument(
        "--scenario",
        type=str,
        default="small",
        choices=["few-shot", "small", "medium", "high"],
        help="ç›®æ ‡åŸŸåœºæ™¯åç§°ï¼ˆfew-shot / small / medium / highï¼‰",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=1088,
        help="é€‰æ‹©å“ªä¸ªå¾®è°ƒ seed çš„ Î¸ ä½œä¸ºå›ºå®šæ£€æµ‹æ¨¡å‹ï¼ˆå¯¹åº” Bias_only/<scenario>/seed_<seed>ï¼‰",
    )
    parser.add_argument(
        "--theta-checkpoint",
        type=str,
        default=None,
        help=(
            "æŒ‡å®šå›ºå®šæ£€æµ‹æ¨¡å‹ Î¸ çš„ best.pt è·¯å¾„ï¼›"
            "è‹¥ä¸æŒ‡å®šï¼Œåˆ™é»˜è®¤ä½¿ç”¨ "
            "Toy/Results/Bias_only/<scenario>/seed_<seed>/results/weights/best.pt"
        ),
    )
    parser.add_argument(
        "--meta-epochs",
        type=int,
        default=1,
        help="åœ¨ meta é›†ä¸Šä¼˜åŒ– g_phi çš„è½®æ•°ï¼ˆæ¯è½®éå†ä¸€æ¬¡ meta dataloaderï¼‰",
    )
    parser.add_argument(
        "--meta-steps",
        type=int,
        default=200,
        help="æ¯ä¸ª meta epoch ä¸­æœ€å¤šä½¿ç”¨å¤šå°‘ä¸ª batchï¼ˆé¿å… meta è®­ç»ƒæ—¶é—´è¿‡é•¿ï¼‰",
    )
    parser.add_argument(
        "--meta-batch-size",
        type=int,
        default=32,
        help="meta dataloader çš„ batch sizeï¼ˆåªå½±å“ G_phi è®­ç»ƒï¼Œä¸å½±å“åŸ YOLO è®­ç»ƒï¼‰",
    )
    parser.add_argument(
        "--lr-phi",
        type=float,
        default=1e-3,
        help="MLP FiLTER (g_phi) çš„å­¦ä¹ ç‡",
    )
    parser.add_argument(
        "--inner-lr",
        type=float,
        default=1e-3,
        help="å†…å±‚è™šæ‹Ÿ GD æ­¥é•¿ï¼Œç”¨äº Î¸ çš„å¤šæ­¥æ›´æ–°",
    )
    parser.add_argument(
        "--inner-steps",
        type=int,
        default=3,
        help="å†…å±‚è™šæ‹Ÿ GD æ­¥æ•° Kï¼ˆè¶Šå¤§è¶Šæ¥è¿‘çœŸå® MWNetï¼Œè®¡ç®—/æ˜¾å­˜å¼€é”€ä¹Ÿè¶Šå¤§ï¼‰",
    )
    parser.add_argument(
        "--lambda-keep-rate",
        type=float,
        default=0.01,
        help="çº¦æŸæƒé‡ w_i ä¸åç¦» 1 çš„ L2 æ­£åˆ™ç³»æ•°ï¼ˆkeep-rate æ­£åˆ™ï¼‰",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    # åŠ è½½é˜¶æ®µä¸€çš„ç»Ÿä¸€é…ç½®ï¼Œè·å–æ•°æ® yamlï¼ˆé»˜è®¤ lambda_bias=1e-4ï¼‰
    cfg = build_bias_finetune_config()
    if args.scenario not in cfg.scenario_data_cfg:
        raise ValueError(f"æœªçŸ¥åœºæ™¯: {args.scenario}")
    data_yaml = cfg.scenario_data_cfg[args.scenario]
    if not data_yaml.exists():
        raise FileNotFoundError(f"æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_yaml}")

    # è§£æ Î¸ çš„ checkpoint è·¯å¾„
    # é»˜è®¤ä¸é˜¶æ®µä¸€ (YOLO + L_bias, Î»_bias=cfg.lambda_bias) çš„ç»“æœç›®å½•ä¿æŒä¸€è‡´ï¼š
    #   cfg.results_root / lambda_xxx / <scenario>/seed_<seed>/results/weights/best.pt
    lambda_dir = "lambda_0" if cfg.lambda_bias == 0 else f"lambda_{cfg.lambda_bias:g}"

    if args.theta_checkpoint is not None:
        theta_ckpt_path = Path(args.theta_checkpoint)
    else:
        theta_ckpt_path = (
            cfg.results_root
            / lambda_dir
            / args.scenario
            / f"seed_{args.seed}"
            / "results"
            / "weights"
            / "best.pt"
        )
    if not theta_ckpt_path.exists():
        raise FileNotFoundError(f"Î¸ checkpoint ä¸å­˜åœ¨: {theta_ckpt_path}")

    # æ„é€ ä¸€ä¸ª DetectionTrainerï¼Œåªä¸ºå¤ç”¨å…¶ dataloader æ„å»ºé€»è¾‘ï¼ˆtrain_loader/test_loaderï¼‰
    # è¿™é‡Œä½¿ç”¨ Î¸ checkpoint ä½œä¸º model æƒé‡ï¼Œç¡®ä¿æ•°æ® pipeline ä¸é˜¶æ®µä¸€ä¸€è‡´ã€‚
    # ä¸é˜¶æ®µä¸‰ (Bias+Filter) è®­ç»ƒçº¦å®šä¿æŒä¸€è‡´ï¼š
    # Bias+Filter ç»“æœç›®å½•æ ¹ä¸º cfg.results_root.parent / "Bias+Filter"
    mt_results_root = cfg.results_root.parent  # .../ICML/mt/Results
    bias_filter_root = mt_results_root / "Bias+Filter" / lambda_dir / args.scenario / f"seed_{args.seed}"

    overrides = build_overrides(
        scenario=args.scenario,
        training_cfg=cfg.training,
        data_yaml=data_yaml,
        project_dir=bias_filter_root / "meta_tmp",
        seed=args.seed,
        model_path=theta_ckpt_path,
    )
    # ä¸º meta è®­ç»ƒè®¾å®šæ›´å°çš„ batch å’Œè¾ƒå°‘çš„ workersï¼Œé¿å…æ˜¾å­˜å‹åŠ›
    overrides["epochs"] = 1
    overrides["batch"] = args.meta_batch_size
    overrides["workers"] = min(overrides.get("workers", 10), 4)
    overrides["project"] = str(bias_filter_root)
    overrides["name"] = "meta_run"

    trainer = DetectionTrainer(overrides=overrides)
    # æ„å»ºæ¨¡å‹ + dataloaderï¼ˆworld_size=0 è§†ä¸ºå• GPU / CPUï¼‰
    trainer._setup_train(world_size=0)

    device = trainer.device
    # DetectionTrainer å†…éƒ¨çš„ model æ˜¯ DetectionModelï¼Œè¿™é‡Œä½œä¸º inner/outer å…±åŒä½¿ç”¨çš„åŸºç¡€æ¨¡å‹
    det_model = trainer.model.to(device)

    # ä½¿ç”¨è®­ç»ƒ dataloader ä½œä¸º inner batch æ¥æºï¼ŒéªŒè¯ dataloader ä½œä¸º meta é›†
    train_loader = trainer.train_loader
    meta_loader = trainer.test_loader
    if train_loader is None:
        raise RuntimeError("æ„å»º train dataloader å¤±è´¥ï¼štrainer.train_loader ä¸ºç©º")
    if meta_loader is None:
        raise RuntimeError("æ„å»º meta dataloader å¤±è´¥ï¼štrainer.test_loader ä¸ºç©º")

    print(
        f"ğŸ§ª å¼€å§‹é˜¶æ®µäºŒ (MWNet-style, multi-step) g_phi è®­ç»ƒï¼šscenario={args.scenario}, seed={args.seed}, "
        f"Î¸={theta_ckpt_path}, train_batchesâ‰ˆ{len(train_loader)}, meta_batchesâ‰ˆ{len(meta_loader)}"
    )

    # æ„å»º MLP FiLTER å’Œ per-sample loss å°è£…
    mlp_filter = MLPFilter().to(device)

    # ä¸º meta é˜¶æ®µåŒæ ·æä¾›é”šç‚¹æ¨¡å‹ Î¸_goodï¼Œç”¨äºæ„é€ ä¸ Î¸_good é¢„æµ‹å·®å¼‚ç›¸å…³çš„ç‰¹å¾
    anchor = None
    try:
        from ICML.core.yolo_bias_finetune.anchor import AnchorModel  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–

        anchor = AnchorModel(cfg.theta_good_path, device=device)
    except Exception:
        anchor = None

    per_sample_loss = PerSampleWeightedDetectionLoss(
        det_model,
        mlp_filter,
        anchor_model=anchor,
    )

    optimizer_phi = torch.optim.Adam(mlp_filter.parameters(), lr=args.lr_phi)

    # meta è®­ç»ƒå¾ªç¯ï¼šåªæ›´æ–° Ï†ï¼Œä¸æ›´æ–° Î¸ï¼ˆdet_modelï¼‰
    # ä½¿ç”¨å¤šæ­¥è¿‘ä¼¼çš„ MWNetï¼šåœ¨å½“å‰ Î¸ ä¸Šæ‰§è¡Œè‹¥å¹²æ­¥è™šæ‹Ÿæ›´æ–° Î¸_kï¼Œç„¶ååœ¨ meta é›†ä¸Šè¯„ä¼° loss(Î¸_K)ï¼Œå¯¹ Ï† åä¼ ã€‚
    inner_lr = args.inner_lr  # å†…å±‚è™šæ‹Ÿæ›´æ–°æ­¥é•¿
    inner_steps = max(int(args.inner_steps), 1)  # è™šæ‹Ÿ inner GD æ­¥æ•°ï¼ˆKï¼‰
    train_iter = iter(train_loader)
    meta_iter = iter(meta_loader)

    for epoch in range(args.meta_epochs):
        print(f"\n[Meta-Epoch {epoch + 1}/{args.meta_epochs}]")
        step = 0
        running_loss = 0.0

        while step < args.meta_steps:
            if step >= args.meta_steps:
                break

            step += 1
            try:
                batch_train = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)
                batch_train = next(train_iter)

            try:
                batch_meta = next(meta_iter)
            except StopIteration:
                meta_iter = iter(meta_loader)
                batch_meta = next(meta_iter)

            batch_train = trainer.preprocess_batch(batch_train)
            batch_meta = trainer.preprocess_batch(batch_meta)

            # 1) innerï¼šåœ¨ train loader ä¸Šæ‰§è¡Œ K æ­¥è™šæ‹Ÿæ›´æ–°ï¼Œå¾—åˆ° Î¸_K
            det_model.train()
            base_named_params = dict(det_model.named_parameters())
            trainable_names = [n for n, p in base_named_params.items() if p.requires_grad]

            # Î¸_0ï¼šä»¥å½“å‰ det_model å‚æ•°ä¸ºèµ·ç‚¹
            theta_k = {n: p for n, p in base_named_params.items()}

            for _ in range(inner_steps):
                # ä¸ºäº†æ›´æ¥è¿‘çœŸå® MWNetï¼Œæ¯ä¸€æ­¥ inner æ›´æ–°ä½¿ç”¨ä¸€ä¸ªæ–°çš„ train batch
                try:
                    batch_train = next(train_iter)
                except StopIteration:
                    train_iter = iter(train_loader)
                    batch_train = next(train_iter)
                batch_train = trainer.preprocess_batch(batch_train)

                # ä½¿ç”¨å½“å‰ Î¸_k è¿›è¡Œä¸€æ¬¡å‰å‘ä¸æŸå¤±è®¡ç®—
                preds_train = functional_call(det_model, theta_k, (batch_train["img"],))
                loss_vec_train, _ = per_sample_loss(preds_train, batch_train)
                inner_loss = loss_vec_train.sum()

                # å¯¹ Î¸_k ä¸­éœ€è¦æ¢¯åº¦çš„å‚æ•°è®¡ç®—ä¸€é˜¶æ¢¯åº¦
                theta_tensors = [theta_k[n] for n in trainable_names]
                grads_theta = torch.autograd.grad(inner_loss, theta_tensors, create_graph=True)

                # æ„é€  Î¸_{k+1}ï¼šGD ä¸€æ­¥æ›´æ–°
                new_theta_k = {}
                grad_iter = iter(grads_theta)
                for name, p in theta_k.items():
                    if name in trainable_names:
                        g = next(grad_iter)
                        new_theta_k[name] = p - inner_lr * g
                    else:
                        new_theta_k[name] = p
                theta_k = new_theta_k

            # 2) outerï¼šåœ¨ meta batch ä¸Šä½¿ç”¨ Î¸_K è®¡ç®—â€œæ›´æ¥è¿‘ mAP çš„â€ meta-lossï¼Œå¹¶å¯¹ Ï† åä¼ 
            det_model.eval()
            preds_meta = functional_call(det_model, theta_k, (batch_meta["img"],))
            loss_vec_meta, _ = per_sample_loss(preds_meta, batch_meta)

            # loss_vec_meta: [box, cls, dfl]
            # å°†åˆ†ç±»éƒ¨åˆ†ä½œä¸ºä¸» meta æŒ‡æ ‡ï¼Œè¾…ä»¥è¾ƒå°æƒé‡çš„ box/dflï¼Œä½œä¸º mAP çš„å¯å¯¼ proxy
            loss_box_meta, loss_cls_meta, loss_dfl_meta = loss_vec_meta
            det_loss = loss_cls_meta + 0.5 * (loss_box_meta + loss_dfl_meta)

            # keep-rate æ­£åˆ™ï¼šé¼“åŠ± w_i ä¸åç¦» 1 å¤ªè¿œï¼Œé¿å…æç«¯ re-weight
            reg = 0.0
            if per_sample_loss.last_weights is not None:
                w = per_sample_loss.last_weights
                reg = args.lambda_keep_rate * ((w - 1.0) ** 2).mean()

            meta_loss = det_loss + reg

            optimizer_phi.zero_grad()
            meta_loss.backward()
            optimizer_phi.step()

            running_loss += float(meta_loss.detach().cpu().item())

            if step % 10 == 0:
                avg = running_loss / step
                print(f"  [meta step {step}/{args.meta_steps}] meta_loss={avg:.4f}")

        avg_epoch_loss = running_loss / max(step, 1)
        print(f"âœ… Meta-Epoch {epoch + 1} å®Œæˆï¼Œå¹³å‡ meta_loss={avg_epoch_loss:.4f}")

    # ä¿å­˜ g_phi (MLP FiLTER) çš„å‚æ•°
    out_dir = TOY_ROOT / "Results" / "Bias+Filter" / args.scenario / f"seed_{args.seed}"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "mlpfilter_meta.pt"
    torch.save(mlp_filter.state_dict(), out_path)
    print(f"\nğŸ’¾ å·²ä¿å­˜ MLP FiLTER (g_phi) å‚æ•°åˆ°: {out_path}")


if __name__ == "__main__":
    main()
