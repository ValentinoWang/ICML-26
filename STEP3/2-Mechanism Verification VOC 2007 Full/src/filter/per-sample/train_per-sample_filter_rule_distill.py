#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Rule distillation for per-sample filter (g_phi) on noisy VOC2007:
- æ•°æ®ï¼švoc07_noisy.yamlï¼ˆå«â€œèƒŒæ™¯æ¯’è¯â€å™ªå£°ï¼‰
- ç‰¹å¾ï¼šPerSampleWeightedDetectionLoss æä¾›çš„ z = [loss_box, loss_cls, loss_dfl, conf_diff, iou_diff]
- è§„åˆ™æ ‡ç­¾ï¼šä½¿ç”¨ loss_sum = loss_box + loss_cls + loss_dflï¼Œå¤§äº loss_thres(é»˜è®¤ 0.3) åˆ¤ä¸ºå™ªå£°æ ·æœ¬ target=0ï¼Œå…¶ä½™ target=1
- è®­ç»ƒï¼šä»…æ›´æ–° g_phiï¼ˆMLPFilterï¼Œå†…éƒ¨å¸¦ BNï¼‰ï¼ŒYOLO Î¸ å›ºå®šï¼ˆä½¿ç”¨ anchor_voc.ptï¼‰ï¼Œæ—©åœ patience=20ï¼Œmax_epochs=10000
- äº§ç‰©ï¼šmlp_filter_voc.pt
"""

from __future__ import annotations

import argparse
import csv
import json
from pathlib import Path
from typing import Dict, Any

import torch
from torch import nn

from ultralytics.models.yolo.detect import DetectionTrainer

# å°† Baseline æ ¹ç›®å½•åŠ å…¥ sys.pathï¼Œå¤ç”¨æ ¸å¿ƒæ¨¡å—
THIS_FILE = Path(__file__).resolve()
ICML_ROOT = THIS_FILE.parents[3]  # /root/autodl-tmp/ICML
BASELINE_ROOT = ICML_ROOT.parent
import sys

if str(BASELINE_ROOT) not in sys.path:
    sys.path.insert(0, str(BASELINE_ROOT))

from ICML.core.yolo_bias_finetune.anchor import AnchorModel
from ICML.core.yolo_bias_finetune.mlp_filter import MLPFilter, PerSampleWeightedDetectionLoss


DEFAULT_ANCHOR_DIR = (
    ICML_ROOT / "2-Mechanism Verification VOC 2007 Full" / "Results" / "Anchor" / "voc07_clean"
)
DEFAULT_DISTILL_DIR = (
    ICML_ROOT / "2-Mechanism Verification VOC 2007 Full" / "Results" / "distill_rule_per-sample" / "voc07_noisy"
)


def build_overrides(
    data_yaml: Path,
    model_path: Path,
    project_dir: Path,
    batch_size: int,
    seed: int,
) -> Dict[str, Any]:
    overrides: Dict[str, Any] = {
        "data": str(data_yaml),
        "model": str(model_path),
        "project": str(project_dir),
        "name": "sample_filter_rule_distill",
        "epochs": 1,  # dataloader æ„å»ºå ä½
        "batch": batch_size,
        "imgsz": 640,
        "workers": 4,
        "seed": seed,
    }
    return overrides


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Rule distillation for g_phi on noisy VOC2007")
    parser.add_argument(
        "--data",
        type=Path,
        default=Path("/root/autodl-tmp/dataset/voc07_noisy_per-sample/voc07_noisy.yaml"),
        help="voc07_noisy.yaml è·¯å¾„ï¼ˆper-sample æ•´å›¾æ¯’ç‰ˆæœ¬ï¼Œé»˜è®¤æŒ‡å‘ dataset å½’æ¡£ä½ç½®ï¼‰",
    )
    parser.add_argument(
        "--anchor",
        type=Path,
        default=None,
        help="å¹²å‡€ VOC è®­ç»ƒå¾—åˆ°çš„ anchor_voc_<model>.ptï¼ˆé»˜è®¤è‡ªåŠ¨åœ¨ Results/Anchor/voc07_clean ä¸‹å–æœ€æ–°ï¼‰",
    )
    parser.add_argument("--batch-size", type=int, default=64, help="è®­ç»ƒ batch size")
    parser.add_argument("--noise-ratio", type=float, default=0.3, help="ä¿ç•™å‚æ•°ï¼Œä¸å†ç”¨äºè§„åˆ™ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰")
    parser.add_argument("--lr-phi", type=float, default=1e-3, help="g_phi å­¦ä¹ ç‡")
    parser.add_argument("--max-epochs", type=int, default=10000, help="æœ€å¤§è½®æ•°ï¼ˆæ—©åœä¸ºä¸»ï¼Œé»˜è®¤ 10000ï¼‰")
    parser.add_argument("--patience", type=int, default=20, help="Loss è¿ç»­å¤šå°‘è½®ä¸ä¸‹é™åˆ™æ—©åœï¼ˆé»˜è®¤ 20ï¼‰")
    parser.add_argument("--seed", type=int, default=1088, help="éšæœºç§å­")
    parser.add_argument(
        "--loss-thres",
        type=float,
        default=0.3,
        help="loss_sum (box+cls+dfl) é˜ˆå€¼ï¼Œå¤§äºåˆ™åˆ¤ä¸ºå™ªå£°æ ·æœ¬ï¼ˆæƒé‡ 0ï¼‰",
    )
    parser.add_argument(
        "--save-path",
        type=Path,
        default=None,
        help="ä¿å­˜ g_phi æƒé‡çš„è·¯å¾„ï¼›è‹¥ä¸æŒ‡å®šï¼Œè‡ªåŠ¨ä¿å­˜åˆ° Results/distill_rule_per-sample/voc07_noisy/mlp_filter_<anchor>.pt",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    if not args.data.exists():
        raise FileNotFoundError(f"æ•°æ® YAML ä¸å­˜åœ¨: {args.data}")

    anchor_path = args.anchor
    if anchor_path is None:
        candidates = sorted(
            DEFAULT_ANCHOR_DIR.glob("anchor_voc_*.pt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
        if not candidates:
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ° anchor æ¨¡å‹ï¼Œè¯·æŒ‡å®š --anchor æˆ–åœ¨ {DEFAULT_ANCHOR_DIR} ä¸‹æ”¾ç½® anchor_voc_*.pt"
            )
        anchor_path = candidates[0]
    if not anchor_path.exists():
        raise FileNotFoundError(f"anchor_voc.pt ä¸å­˜åœ¨: {anchor_path}")

    save_path = args.save_path
    if save_path is None:
        tag = anchor_path.stem.replace("anchor_", "")
        save_path = DEFAULT_DISTILL_DIR / f"mlp_filter_{tag}.pt"
    save_path.parent.mkdir(parents=True, exist_ok=True)

    print("ğŸ§ª Train g_phi (sample filter) via rule distillation on noisy VOC2007")
    print(f"   data: {args.data}")
    print(f"   anchor: {anchor_path}")
    print(f"   save_path: {save_path}")
    print(f"   batch_size: {args.batch_size}, noise_ratio: {args.noise_ratio}")
    print(f"   lr_phi: {args.lr_phi}, max_epochs: {args.max_epochs}, patience: {args.patience}")

    project_dir = save_path.parent  # ä¸å†åˆ›å»ºé¢å¤–å­ç›®å½•ï¼Œç›´æ¥åœ¨ distill æ ¹ç›®å½•ä¸‹è®°å½•
    overrides = build_overrides(
        data_yaml=args.data,
        model_path=anchor_path,
        project_dir=project_dir,
        batch_size=args.batch_size,
        seed=args.seed,
    )
    trainer = DetectionTrainer(overrides=overrides)
    trainer._setup_train(world_size=0)
    run_dir = save_path.parent  # ä¸æƒé‡åŒçº§ï¼Œé¿å…é¢å¤– runs_* ç›®å½•
    run_dir.mkdir(parents=True, exist_ok=True)
    log_path = run_dir / "train_log.csv"
    args_path = run_dir / "args.json"

    meta = {
        "data": str(args.data),
        "anchor": str(anchor_path),
        "save_path": str(save_path),
        "batch_size": args.batch_size,
        "noise_ratio": args.noise_ratio,
        "lr_phi": args.lr_phi,
        "max_epochs": args.max_epochs,
        "patience": args.patience,
        "seed": args.seed,
        "loss_thres": args.loss_thres,
    }
    with args_path.open("w") as f:
        json.dump(meta, f, indent=2)

    device = trainer.device
    det_model = trainer.model.to(device)
    for p in det_model.parameters():
        p.requires_grad_(False)
    det_model.eval()

    anchor = AnchorModel(anchor_path, device=device)
    mlp_filter = MLPFilter(device=device).to(device)
    per_sample_loss = PerSampleWeightedDetectionLoss(
        det_model,
        sample_filter=None,
        anchor_model=anchor,
    )

    optimizer = None  # å»¶è¿Ÿåˆ°ç¬¬ä¸€æ¬¡å‰å‘åå†åˆå§‹åŒ–ï¼Œé¿å…ç©ºå‚æ•°åˆ—è¡¨

    train_loader = trainer.train_loader
    if train_loader is None:
        raise RuntimeError("train_loader ä¸ºç©ºï¼Œæ£€æŸ¥æ•°æ®é…ç½®")

    best_loss = float("inf")
    patience_left = args.patience
    torch.manual_seed(args.seed)
    history = []
    bce = torch.nn.BCELoss()

    for epoch in range(args.max_epochs):
        mlp_filter.train()
        running = 0.0
        batches = 0
        noisy_samples = 0
        total_samples = 0

        for batch in train_loader:
            batch = trainer.preprocess_batch(batch)
            imgs = batch["img"].to(device)
            preds = det_model(imgs)

            _, _, feats = per_sample_loss(preds, batch, return_features=True)
            if feats is None:
                continue

            # è§„åˆ™æ‰“åˆ†ï¼šä½¿ç”¨ batch å†…æ‰‹å·¥å½’ä¸€åŒ–ï¼Œé¿å…ç”¨å°šæœªæ”¶æ•›çš„ BN ç»Ÿè®¡é‡
            feats = feats.to(device)
            if feats.shape[0] < 2:
                # BN åœ¨ batch=1 æ—¶ä¼šæŠ¥é”™ï¼Œè·³è¿‡è¿‡å°æ‰¹æ¬¡
                continue
            loss_sum = feats[:, 0] + feats[:, 1] + feats[:, 2]
            target = torch.ones_like(loss_sum)
            noisy_mask = loss_sum > args.loss_thres
            target[noisy_mask] = 0.0  # æ–©ç«‹å†³ï¼Œå®Œå…¨åˆ‡æ–­å™ªå£°æ¢¯åº¦
            noisy_samples += int(noisy_mask.sum().item())
            total_samples += noisy_mask.numel()

            # å­¦ç”Ÿå‰å‘ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œç”±å†…éƒ¨ BN å¤„ç†
            pred_w = mlp_filter(feats).view(-1)

            # é¦–æ¬¡æ‹¿åˆ°æœ‰æ•ˆå‚æ•°åå†åˆ›å»ºä¼˜åŒ–å™¨
            if optimizer is None:
                params = [p for p in mlp_filter.parameters() if p.requires_grad]
                if not params:
                    # è‹¥ä»æœªæ„å»ºç½‘ç»œï¼Œåˆ™è·³è¿‡æœ¬ batch
                    continue
                optimizer = torch.optim.Adam(params, lr=args.lr_phi)
            target = target.to(dtype=pred_w.dtype)
            loss = bce(pred_w, target)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running += loss.item()
            batches += 1

        epoch_loss = running / max(batches, 1)
        improved = epoch_loss + 1e-6 < best_loss
        if improved:
            best_loss = epoch_loss
            patience_left = args.patience
            torch.save(mlp_filter.state_dict(), save_path)
        else:
            patience_left -= 1
            if patience_left <= 0:
                print(f"[EarlyStop] no improvement for {args.patience} epochs, stop at {epoch+1}")
                break

        history.append(
            {
                "epoch": epoch + 1,
                "loss": epoch_loss,
                "best_loss": best_loss,
                "improved": improved,
                "patience_left": patience_left,
                "noisy_samples": noisy_samples,
                "total_samples": total_samples,
            }
        )
        print(
            f"[Epoch {epoch+1}] loss={epoch_loss:.6f} "
            f"(best={best_loss:.6f}, improved={improved}, patience_left={patience_left}, "
            f"noisy={noisy_samples}/{total_samples})"
        )

    print(f"âœ… g_phi saved to {save_path}")
    if history:
        with log_path.open("w", newline="") as f:
            fieldnames = [
                "epoch",
                "loss",
                "best_loss",
                "improved",
                "patience_left",
                "noisy_samples",
                "total_samples",
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(history)
        print(f"ğŸ“„ Training log saved to {log_path}")
    else:
        print("âš ï¸ No training steps were recorded; log file was not written.")


if __name__ == "__main__":
    main()
